{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Multi-Layer Perceptrons\n",
    "\n",
    "In this lab, we aim to solve the problem of classifying human faces into males and females. We have at our disposition a dataset of 200 images of celebrity faces and their associated labels (0 for female and 1 for male).  \n",
    "\n",
    "We  use a face detector from the dlib library to  estimate  the  location  of  68  (x,  y) coordinates  that  map  to specific facial regions. The image below visualizes what each of these coordinates maps to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](face_feature_extraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use tensorflow to define and train a multi-layer perceptron (MLP) graph to classify images using the features visualized above. Using the code below, try to apply the following changes:\n",
    "    \n",
    "1 - Change the complexity of the 2-layer MLP by increasing or decreasing the number of neurons in each layer.\n",
    "\n",
    "2 - Try increasing the number of layers used. This should increase the \"depth\" of the MLP. To do so, you must change the definition of the MLP function in \"multilayer_perceptron\" and the weights/biases allocation function \"allocate_weights_and_biases\".\n",
    "\n",
    "3 - Try changing how the weights and parameters are initialized. What would happen if you initialize all parameters to zero ?\n",
    "\n",
    "4 - Try increasing or decreasing the learning rate and number of training epochs. How does this affect the \"fitting\" to training data ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import APIs to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import lab3_data as import_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load  CelebA data and create train and test splits (Train: 100 exmaples, Test: 100 examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    X, y = import_data.extract_features_labels()\n",
    "    Y = np.array([y, -(y - 1)]).T\n",
    "    tr_X = X[:100] ; tr_Y = Y[:100]\n",
    "    te_X = X[100:] ; te_Y = Y[100:]\n",
    "\n",
    "    return tr_X, tr_Y, te_X, te_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Allocate memory for weights and biases for all MLP layers\n",
    "You can try changing the number of neurons to increase or decrease the complexity of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allocate_weights_and_biases():\n",
    "\n",
    "    # define number of hidden layers ..\n",
    "    n_hidden_1 = 2048  # 1st layer number of neurons\n",
    "    n_hidden_2 = 2048  # 2nd layer number of neurons\n",
    "\n",
    "    # inputs placeholders\n",
    "    X = tf.placeholder(\"float\", [None, 68, 2])\n",
    "    Y = tf.placeholder(\"float\", [None, 2])  # 2 output classes\n",
    "    \n",
    "    # flatten image features into one vector (i.e. reshape image feature matrix into a vector)\n",
    "    images_flat = tf.contrib.layers.flatten(X)  \n",
    "    \n",
    "    # weights and biases are initialized from a normal distribution with a specified standard devation stddev\n",
    "    stddev = 0.01\n",
    "    \n",
    "    # define placeholders for weights and biases in the graph\n",
    "    weights = {\n",
    "        'hidden_layer1': tf.Variable(tf.random_normal([68 * 2, n_hidden_1], stddev=stddev)),\n",
    "        'hidden_layer2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=stddev)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, 2], stddev=stddev))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bias_layer1': tf.Variable(tf.random_normal([n_hidden_1], stddev=stddev)),\n",
    "        'bias_layer2': tf.Variable(tf.random_normal([n_hidden_2], stddev=stddev)),\n",
    "        'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
    "    }\n",
    "    \n",
    "    return weights, biases, X, Y, images_flat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Define how the weights and biases are used for inferring classes from inputs (i.e. define MLP function)\n",
    "\n",
    "You can add more layers to the MLP to fit more complicated functions. Adding more layers requires more learnable weights and biases, which need to defined in \"allocate_weights_and_biases\" first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron():\n",
    "        \n",
    "    weights, biases, X, Y, images_flat = allocate_weights_and_biases()\n",
    "\n",
    "    # Hidden fully connected layer 1\n",
    "    layer_1 = tf.add(tf.matmul(images_flat, weights['hidden_layer1']), biases['bias_layer1'])\n",
    "    layer_1 = tf.math.sigmoid(layer_1)\n",
    "\n",
    "    # Hidden fully connected layer 2\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['hidden_layer2']), biases['bias_layer2'])\n",
    "    layer_2 = tf.math.sigmoid(layer_2)\n",
    "    \n",
    "    # Output fully connected layer\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer, X, Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define graph training operation\n",
    "The loss function (i.e. the value to minimize) is defined as the cross entropy between the predicted classes and the class ground truth. The train operation is then included within the graph as a weight/bias update operation.\n",
    "\n",
    "Try changing the learning rate, how would setting a low or high learning rate affect the \"fitting\" to the training set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "learning_rate = 0.00001\n",
    "training_epochs = 500\n",
    "\n",
    "# display training accuracy every ..\n",
    "display_accuracy_step = 2\n",
    "\n",
    "    \n",
    "training_images, training_labels, test_images, test_labels = get_data()\n",
    "logits, X, Y = multilayer_perceptron()\n",
    "\n",
    "# define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# define training graph operation\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# graph operation to initialize all variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run graph for specified number of epochs.\n",
    "\n",
    "After the graph is defined, different operations in the graph can be run by specifying them in the sess.run() function.\n",
    "A session is wrapper for running graphs. Outputs can also be acquired from the graph by including them in the variable list of sess.run()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "        # run graph weights/biases initialization op\n",
    "        sess.run(init)\n",
    "        # begin training loop ..\n",
    "        for epoch in range(training_epochs):\n",
    "            # run optimization operation (backprop) and cost operation (to get loss value)\n",
    "            _, cost = sess.run([train_op, loss_op], feed_dict={X: training_images,\n",
    "                                                               Y: training_labels})\n",
    "\n",
    "            # Display logs per epoch step\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost={:.9f}\".format(cost))\n",
    "                \n",
    "            if epoch % display_accuracy_step == 0:\n",
    "                pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "                correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "\n",
    "                # calculate training accuracy\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                print(\"Accuracy: {:.3f}\".format(accuracy.eval({X: training_images, Y: training_labels})))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # -- Define and run test operation -- #\n",
    "        \n",
    "        # apply softmax to output logits\n",
    "        pred = tf.nn.softmax(logits)\n",
    "        \n",
    "        #  derive inffered calasses as the class with the top value in the output density function\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        # run test accuracy operation ..\n",
    "        print(\"Test Accuracy:\", accuracy.eval({X: test_images, Y: test_labels}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
