{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "* To train a Deep Reinforcement Learning model (DQN) to solve the Gridworld game, in which a player collects objects in a room.\n",
    "* To build a solid understanding of the components  of  DQNs.\n",
    "* To practice optimization techniques for training deep neural networks efficiently.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description\n",
    "\n",
    "We aim to train an agent to collect an object in a room. The room specifications are decided by the user (room dimensions and locations of inner walls). The agent is initially placed at a fixed or random starting location in the room. At each time step, the agent takes one of four actions (move north, east, south or west). Every time the agent bumps into a wall, it receives a negative reward of $-1$. When the agent finds the goal object, it receives a positive reward of $10$ and the game terminates. After $100$ steps, if the agent has not reached the goal yet, the game terminates.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started \n",
    "\n",
    "We provide you with the code for buiding the Gridwold game environment. The relevent Python files can be found in the home folder in `Prefabs/`, `game.py`, and `build_game.py`.\n",
    "\n",
    "Below you can find an example of how to build and start a game. The objective of this exercise will be to build a Q-Network to learn the optimal policy. To that effect, we provide you with some initial code and you are expected to complete the code, filling in the blanks highlighted by TODO tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "This hands-on exercice requires the following standard Python libraries: numpy, time, matplotlib, os, random scipy as well as tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import build_game\n",
    "from Prefabs import static\n",
    "from Prefabs.player import NormalPlayer\n",
    "from Prefabs.interactive import GoldObj\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the GridWorld Environment \n",
    "\n",
    "To design a GridWorld environment with a fixed initial state, we use the function `build_game.build_game()`. It requires to give as input a \"drawing\" of the initial state as well as some object information. In the example below, the walls are drawn with the character '#', the player with character 'P' and the target object with character 'I'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ascii_art_world = ['######',\n",
    "                   '# I  #',\n",
    "                   '#    #',\n",
    "                   '#  P #',\n",
    "                   '#    #',\n",
    "                   '######']\n",
    "\n",
    "obj_information= {\n",
    "    'P': NormalPlayer((0, 0, 255)),\n",
    "    '#': static.Static((0, 0, 0)),\n",
    "    'I': GoldObj((0, 255, 255))\n",
    "}\n",
    "\n",
    "obj_size = 2\n",
    "\n",
    "env = build_game.build_game(ascii_art_world, obj_information, obj_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Hpyerparameters of the Q-Network and the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # How many experiences to use for each training step.\n",
    "update_freq = 4 # How often to perform a training step.\n",
    "y = .99 # Discount factor on the target Q-values\n",
    "startE = 1 # Starting chance of random action\n",
    "endE = 0.1 # Final chance of random action\n",
    "anneling_steps = 3000 # How many steps of training to reduce startE to endE.\n",
    "num_episodes = 300 # How many episodes of game environment to train network with.\n",
    "pre_train_steps = 500 # How many steps of random actions before training begins.\n",
    "model_path = \"./models/dqn\" # The path to save our model to.\n",
    "summary_path = './summaries/dqn' # The path to save summary statistics to.\n",
    "h_size = 256 # The number of units in the hidden layer.\n",
    "learning_rate = 1e-3 # Agent Learning Rate\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model\n",
    "C = 1000 # After how many steps the target network is updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will build to approximate the Q value function is dueling DQN, i.e. a neural network consisting of two convlutional layers and some fully connected layers. It is depicted below.\n",
    "\n",
    "![alt text](Qnet.jpeg \"Q-Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below implements the major parts of the Qnetwork. Within the code, you will find some missing parts, your task is to implement them. They are summarised here (it might be useful to have a look at the last slides of today's lecture to have an idea of the mathematical steps that you need to implement):\n",
    "\n",
    "**Task 1:** Write the code to compute the Q-values (output of the network) given the advantage and the value function.  \n",
    "**Task 2:** Write the code to predict the best action given the Q-values.  \n",
    "**Task 3:** Write the code to compute the TD error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self ,h_size, num_actions, lr, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            # The network recieves a frame from the game, flattened into an array.\n",
    "            # It then resizes it and processes it through four convolutional layers.\n",
    "            self.observation_input =  tf.placeholder(shape=[None, 12, 12, 3],dtype=tf.float32)\n",
    "            self.conv1 = slim.conv2d(self.observation_input, 32, \n",
    "                                     kernel_size=[3,3], stride=[2,2], \n",
    "                                     biases_initializer=None,\n",
    "                                     activation_fn=tf.nn.elu)\n",
    "            self.conv2 = slim.conv2d(self.conv1, 64, \n",
    "                                     kernel_size=[3,3], \n",
    "                                     stride=[2,2], \n",
    "                                     biases_initializer=None,\n",
    "                                     activation_fn=tf.nn.elu)\n",
    "\n",
    "            # We take the output from the final convolutional layer \n",
    "            # and split it into separate advantage and value streams.\n",
    "            self.hidden = slim.fully_connected(slim.flatten(self.conv2), \n",
    "                                               h_size, activation_fn=tf.nn.elu)\n",
    "            self.advantage = slim.fully_connected(self.hidden, num_actions, activation_fn=None,\n",
    "                                                  biases_initializer=None)\n",
    "            self.value = slim.fully_connected(self.hidden, 1, activation_fn=None,\n",
    "                                                  biases_initializer=None)\n",
    "\n",
    "            # TODO - Task 1: Combine advantage and vaule together to get the final Q-values.\n",
    "            self.q_out = ... \n",
    "            \n",
    "            # TODO - Task 2: Select the best action given q_out\n",
    "            self.predict = ...\n",
    "\n",
    "            # Below we obtain the loss by taking the sum of squares difference \n",
    "            # between the target and prediction Q values.\n",
    "            self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,num_actions,dtype=tf.float32)\n",
    "\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.q_out, self.actions_onehot), axis=1)\n",
    "\n",
    "            # TODO - Task 3: Compute the TD error\n",
    "            self.td_error = ...\n",
    "            self.loss = tf.reduce_mean(self.td_error)\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            self.update = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay \n",
    "\n",
    "The code below implements the class for storing past experiences and for sampling from them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Target Network\n",
    "\n",
    "The code below implements the function that updates the parameters of a network (the target network) with the parameters of another network (the main network). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_target_graph(from_scope, to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Q-Network\n",
    "\n",
    "We can now start training the network. The code below implements the main training parts, but is missing some key elements. Similarly as before, fill in the blanks.\n",
    "\n",
    "**Task 4:** Implement the epsilon-greedy strategy. In other words, write the code to select a random action with probability epsilon, otherwise select the best action given the Q network.  \n",
    "**Task 5:** Write the code to retrieve a batch of random samples from the buffer (hint: you can use the functions defined in the `experience_buffer` class).  \n",
    "**Task 6:** Write the code to compute the episode's reward (cumulative steps rewards). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now completed your first deep RL implementation! You can now run the cell to start the training. You will see the evolution of the agent's policy in the interactive plot below the cell while the code is being executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Display the environement in a figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "plt.ion()\n",
    "\n",
    "ax.imshow(env.render_map())\n",
    "ax.set_title('reward: %.2f' % 0., loc='right', fontsize=18)\n",
    "ax.set_title('epoch: %d' % 0, loc='left', fontsize=18)\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "# ***************************************\n",
    "\n",
    "# Create the folder for saving the models\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "    \n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "    \n",
    "action_space_size = 4\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Initialise the main Qnet ant the target Qnet\n",
    "mainQN = Qnetwork(h_size, action_space_size, learning_rate, \"main\")\n",
    "targetQN = Qnetwork(h_size, action_space_size, learning_rate, \"target\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "update_target_ops = update_target_graph(\"main\", \"target\")\n",
    "\n",
    "# Initialise the replay memory buffer  \n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# Set the rate of random action decrease. \n",
    "epsilon = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "episode_lengths = []\n",
    "episode_rewards = []\n",
    "losses = []\n",
    "total_steps = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for episode in range(num_episodes):\n",
    "        time.sleep(0.3)\n",
    "        episodeBuffer = experience_buffer()\n",
    "        observations = env.reset()\n",
    "        observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        \n",
    "        # Update the environement display\n",
    "        ax.clear()\n",
    "        ax.imshow(env.render_map())\n",
    "        ax.set_title('reward: %.2f' % episode_reward, loc='right', fontsize=18)\n",
    "        ax.set_title('epoch: %d' % episode, loc='left', fontsize=18)\n",
    "        fig.canvas.draw()\n",
    "        # *******************************\n",
    "        \n",
    "        while not done:\n",
    "            # TODO - Task 4: Choose an action by greedily (with espilon chance of random action) from the Q-network\n",
    "            action = ...\n",
    "            \n",
    "            # if we are not in training mode, we always pick the best action given the network\n",
    "            if not train_model :\n",
    "                action = sess.run(mainQN.predict, \n",
    "                                  feed_dict={mainQN.observation_input:[observation]})[0]\n",
    "            new_observations, reward, done = env.step(action)\n",
    "                                      \n",
    "            new_observation = observation[:, :, 1:]\n",
    "            new_observation = np.concatenate([new_observation, new_observations], axis=2)\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Save the experience to our episode buffer.\n",
    "            episodeBuffer.add(np.reshape(np.array([observation,action,reward,new_observation,done]),[1,5])) \n",
    "            \n",
    "            if total_steps > pre_train_steps and train_model:\n",
    "                if total_steps % C == 0:\n",
    "                    sess.run(update_target_ops)\n",
    "                \n",
    "                # Decay the espsilon parameter\n",
    "                if epsilon > endE:\n",
    "                    epsilon -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    # TODO - Task 5: Get a random batch of experiences.\n",
    "                    trainBatch = ... \n",
    "                    # Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict, \n",
    "                                  feed_dict={mainQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "                    Q2 = sess.run(targetQN.q_out, \n",
    "                                  feed_dict={targetQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    # Update the network with our target values.\n",
    "                    _, q_loss = sess.run([mainQN.update, mainQN.loss],\n",
    "                        feed_dict={mainQN.observation_input:np.stack(trainBatch[:,0], axis=0),\n",
    "                                   mainQN.targetQ:targetQ, \n",
    "                                   mainQN.actions:trainBatch[:,1]})\n",
    "                    losses.append(q_loss)\n",
    "            \n",
    "            # TODO - Task 6: Compute the current cumulative reward\n",
    "            episode_reward = ...\n",
    "            episode_steps += 1\n",
    "            observation = new_observation\n",
    "            \n",
    "            # Update the environement display\n",
    "            ax.clear()\n",
    "            ax.imshow(env.render_map())\n",
    "            ax.set_title('reward: %.2f' % episode_reward, loc='right', fontsize=18)\n",
    "            ax.set_title('epoch: %d' % episode, loc='left', fontsize=18)\n",
    "            fig.canvas.draw()\n",
    "            # ********************************\n",
    "        \n",
    "        # Save the episode buffer to the overall buffer .\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        episode_lengths.append(episode_steps)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Periodically save the model and print statistics\n",
    "        if episode % 1000 == 0 and episode != 0:\n",
    "            saver.save(sess, model_path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")\n",
    "        if episode % 10 == 0 and episode != 0:\n",
    "            print (\"Mean Reward: {}\".format(np.mean(episode_rewards[-10:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Observe the agent improving its policy over time. After a few training epochs, the agent should be making less mistakes (such as bumping into walls) and should go more directly towards the goal. \n",
    "\n",
    "**Task 7** How many episodes does the network take to train? (Hint: look at the reward increment in the learning.)  \n",
    "\n",
    "Once the training is completed, run the cell below. It will plot the cumulative reward of each training espisode over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(episode_rewards)\n",
    "plt.ylabel('Episode cumulative reward')\n",
    "plt.xlabel('Number of training episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Changing the Game Settings\n",
    "We are now intersted in understating if the same network is able to perform well in different environement settings. We consider the same game, but with a random initialisation. In other words, at each new episode, both the player and the goal object are placed at a new random location. \n",
    "\n",
    "Think about the following:\n",
    "* How does it affect the state space? The action space?\n",
    "* How do we expect the network to behave? \n",
    "\n",
    "**Task 8**: Go back to the designing the GridWorld cell and do the necessary changes in the code to build a random GridWorld game.  \n",
    "Hint: The function \n",
    "```python\n",
    "build_game.build_random_game(grid_size, obj_information, obj_size)\n",
    "``` \n",
    "returns a grid word environement of dimentions grid_size X grid_size.  \n",
    "**Task 9**: Run the training again for the same number of episodes found in Task 7. Run the next cell and observe the evolution of the training now compared to the previous settings. Is the network fully trained now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(episode_rewards)\n",
    "plt.ylabel('Episode cumulative reward')\n",
    "plt.xlabel('Number of training episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 10**: How can we improve the learning? (Hint: did the agent explore the environement enough?)  \n",
    "To validate your hypothesis, you can run a more efficient training and plot again the cumulative reward vs the training episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the Q-Network Settings\n",
    "We now investigate how the network's architecture affects the learnt policy. \n",
    "\n",
    "**Task 11**: In the QNetwork class implementation, remove one of the convolutional layers. Run the training again and observe the changes in how the training evolves over time.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(episode_rewards)\n",
    "plt.ylabel('Episode cumulative reward')\n",
    "plt.xlabel('Number of training episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 12**: In additon of the number of layers, there are many other network parameters that are pre-defined. \n",
    "For each of the following parameters, change their value and train the network again. Each time, observe how it affects the performance of the learning  (i.e., plot the cumulative reward vs the training episodes).   \n",
    "• Number of hidden units in the fully connected layers.  \n",
    "• Type of activation function at the output of the convlutional layers (currently it is the exponential linear given by `tf.nn.elu`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(episode_rewards)\n",
    "plt.ylabel('Episode cumulative reward')\n",
    "plt.xlabel('Number of training episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Changing the Training Settings\n",
    "**Task 13**: The training itself also requires some predefined parameters such as the value of epsilon for performing espilon-greedy action selection, the discount factor and experience replay. For each of the following parameters, change their value and train the network again. Each time, observe how it affects the performance of the learning.   \n",
    "* The anneling step; how quickly/slowly to decay the espilon parameter. This relates to the exploration/exploitation tradeoff.  \n",
    "* The batch size; how many experiences to use for each training step.  \n",
    "* Training without experience replay. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(episode_rewards)\n",
    "plt.ylabel('Episode cumulative reward')\n",
    "plt.xlabel('Number of training episodes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
